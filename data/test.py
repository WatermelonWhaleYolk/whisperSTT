from transformers import WhisperProcessor, WhisperForConditionalGeneration
import torch
import librosa

# 1. ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
model_dir = "./whisper-finetuned-final"
processor = WhisperProcessor.from_pretrained(model_dir)
model = WhisperForConditionalGeneration.from_pretrained(model_dir)

# âœ… ì´ê±° ê¼­ í•´ì•¼ ë¨! ì´ê±° ì—†ìœ¼ë©´ ì˜¤ë¥˜ ê·¸ëŒ€ë¡œ ë°œìƒí•¨
model.config.forced_decoder_ids = None
model.generation_config.forced_decoder_ids = None  # ì´ê±´ ì¶”ê°€ë¡œ ì•ˆì „ë§

# 2. ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
audio_path = "test_korean_audio.wav"
audio, sr = librosa.load(audio_path, sr=16000)

# 3. 30ì´ˆ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
chunk_size = 16000 * 30
chunks = [audio[i:i + chunk_size] for i in range(0, len(audio), chunk_size)]

results = []

# 4. ì˜ˆì¸¡ ë°˜ë³µ
for chunk in chunks:
    inputs = processor(chunk, sampling_rate=16000, return_tensors="pt")
    input_features = inputs["input_features"]

    with torch.no_grad():
        predicted_ids = model.generate(input_features)

    text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    results.append(text)

# 5. ê²°ê³¼ ì¶œë ¥
print("\nğŸ“ ì „ì²´ Transcription:")
print(" ".join(results))
