import transformers
from transformers import TrainingArguments
print("ğŸ”¥ transformers version:", transformers.__version__)
print("ğŸ”¥ TrainingArguments module:", TrainingArguments.__module__)
print("ğŸ”¥ transformers location:", transformers.__file__)
# ===================================

from datasets import load_dataset, Audio
from transformers import WhisperProcessor, WhisperForConditionalGeneration, Trainer
import torch
import os

# 1. ë°ì´í„° ë¡œë“œ ë° ê²½ë¡œ ì •ë¦¬
dataset = load_dataset("csv", data_files={"train": "segments_dataset.csv"}, delimiter=",")

def fix_path(example):
    example["audio"] = example["audio"].replace("\\", "/")  # ìœˆë„ìš° ê²½ë¡œ ìˆ˜ì •
    return example

dataset["train"] = dataset["train"].map(fix_path)

# 2. ì˜¤ë””ì˜¤ ì»¬ëŸ¼ í˜• ë³€í™˜
dataset["train"] = dataset["train"].cast_column("audio", Audio(sampling_rate=16000))

# 3. ì´ì „ì— íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model_path = "../zeraV03"
processor = WhisperProcessor.from_pretrained(model_path)
model = WhisperForConditionalGeneration.from_pretrained(model_path)

# 4. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def preprocess(example):
    audio = example["audio"]["array"]
    inputs = processor.feature_extractor(audio, sampling_rate=16000)
    input_features = inputs["input_features"][0]
    labels = processor.tokenizer(example["transcription"]).input_ids
    return {
        "input_features": input_features,
        "labels": labels
    }

# 5. ì „ì²˜ë¦¬ ì‹¤í–‰
dataset = dataset["train"].map(preprocess)

# 6. ë°ì´í„° ì½œë ˆì´í„° ì •ì˜
def data_collator(batch):
    input_features = torch.tensor([example["input_features"] for example in batch])
    labels = [torch.tensor(example["labels"]) for example in batch]
    labels = torch.nn.utils.rnn.pad_sequence(
        labels, batch_first=True, padding_value=processor.tokenizer.pad_token_id
    )
    return {"input_features": input_features, "labels": labels}

# 7. í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="../zeraV04",  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í´ë”
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=2e-5,
    warmup_steps=100,
    max_steps=6000,
    logging_steps=50,
    save_steps=2000,
    save_total_limit=2,
)

# 8. Trainer êµ¬ì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=processor,  # âœ… ë¬¸ì œ í•´ê²°: feature_extractorê°€ ì•„ë‹Œ processor ì „ì²´ ë„˜ê¸°ê¸°
    data_collator=data_collator,
)

# 9. í•™ìŠµ ì‹¤í–‰
trainer.train()